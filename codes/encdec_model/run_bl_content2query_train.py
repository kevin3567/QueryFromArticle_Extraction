import json
from pathlib import Path
from transformers import AutoTokenizer
import torch
from transformers import EncoderDecoderModel
from torch.utils.data import DataLoader
from transformers import AdamW
import numpy as np
from hanziconv import HanziConv
import argparse
import time
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments


### Retrieve load the dataset from the corresponding files
def get_dataset(queryfile, contentfile):
    with open(queryfile, "r", encoding="utf-8") as fd:
        querys = [line.rstrip() for line in fd.readlines()]

    with open(contentfile, "r", encoding="utf-8") as fd:
        contents = [line.rstrip() for line in fd.readlines()]

    assert len(querys) == len(contents), \
        "Data Fields (Files) have different lengths"

    return querys, contents


def show_answer(tokenizer_model, in_encodings, out_encodings, idx):
    print("Index :: ", idx)
    print("Input (Content) :: ", tokenizer_model.decode(in_encodings['input_ids'][idx]))
    print("Output (Query) :: ", tokenizer_model.decode(out_encodings['input_ids'][idx]))
    print("")


class GenerationDataset(torch.utils.data.Dataset):  # torch dataset object
    def __init__(self, input_encodings,
                 output_encodings):  # takes in encoded datasets generated by huggingface tokenizers
        assert len(input_encodings) == len(output_encodings), "Input and Output are not the same size"
        self.input_encodings = input_encodings
        self.output_encodings = output_encodings

    def __getitem__(self, idx):  # for each idx, pass the input encodings and output encodings
        input_dict = {key: torch.tensor(val[idx]) for key, val in self.input_encodings.items()}
        output_dict = {key: torch.tensor(val[idx]) for key, val in self.output_encodings.items()}
        return input_dict, output_dict

    def __len__(self):  # get the length (number of samples) in the dataset
        return len(self.input_encodings.input_ids)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Training Arguments")
    parser.add_argument("--contentfile", type=str, required=True, help="Content File (Input)")
    parser.add_argument("--queryfile", type=str, required=True, help="Query File (Output)")

    parser.add_argument("--batch_size", type=int, default=16, help="Batch Size")
    parser.add_argument("--lr", type=float, default=5e-5, help="Learning Rate (5e-5 for Train / 1e-5 for Finetune)")
    parser.add_argument("--epochs", type=int, default=25, help="Train/Finetune Max Epochs")
    parser.add_argument("--modelckpt", type=str, default="", help="EncDecModel Checkpoint (If Provided)")
    parser.add_argument("--savepath", type=str, default="models/", help="Model Save Path")
    args = parser.parse_args()
    print("Get all Arguments:")
    for arg in vars(args):
        print("{}: {}".format(arg, getattr(args, arg)))

    modelbase = "bert-base-chinese"

    train_querys, train_contents = get_dataset(args.queryfile,
                                               args.contentfile)

    tokenizer = AutoTokenizer.from_pretrained(modelbase)
    tokenizer.add_special_tokens({"additional_special_tokens": ["[unused1]"]})
    train_input_encodings = tokenizer(train_contents, truncation=True, padding=True)
    train_output_encodings = tokenizer(train_querys, truncation=True, padding=True)

    print("Show some examples: ")
    show_answer(tokenizer, train_input_encodings, train_output_encodings, 0)
    # show_answer(tokenizer, train_input_encodings, train_output_encodings, 100)
    # show_answer(tokenizer, train_input_encodings, train_output_encodings, 2000)
    show_answer(tokenizer, train_input_encodings, train_output_encodings, -1)
    print("Length of Train Set: {}".format(len(train_contents)))
    print("Done Dataset Processing")

    """The dataset is now ready for training"""
    train_dataset = GenerationDataset(train_input_encodings, train_output_encodings)
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)

    if args.modelckpt != "":
        print("Load from modelckpt: {}".format(args.modelckpt))
        model = EncoderDecoderModel.from_pretrained(args.modelckpt)
    else:
        print("Build from pretrained model: {}".format(modelbase))
        model = EncoderDecoderModel.from_encoder_decoder_pretrained(modelbase,
                                                                    modelbase)  # bert2bert, chinese variant
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)
    model.train()
    print("Model Configuration")
    print(model.config)
    print("Model Architecture")
    print(model)

    optim = AdamW(model.parameters(), lr=args.lr)

    epoch, step = 0, 0
    for epoch in range(args.epochs):
        start = time.time()
        for step, (batch_in, batch_out) in enumerate(train_loader):
            optim.zero_grad()
            input_ids = batch_in["input_ids"].to(device)
            attention_mask = batch_in["attention_mask"].to(device)
            decoder_input_ids = batch_out["input_ids"].to(device)
            decoder_attention_mask = batch_out["attention_mask"].to(device)
            labels = batch_out["input_ids"].to(device)
            outputs = model(input_ids=input_ids,
                            attention_mask=attention_mask,
                            decoder_input_ids=decoder_input_ids,
                            decoder_attention_mask=decoder_attention_mask,
                            labels=labels)
            loss = outputs[0]
            loss.backward()
            optim.step()
            if ((step + 1) % 200) == 0:
                end = time.time()
                print(
                    "Epoch: {}, Step: {}, Duration (From Last Print-Out): {}".format(epoch + 1, step + 1, end - start))
                start = time.time()
        end = time.time()
        print("Epoch: {}, Step: {}, Duration (From Last Print-Out): {}".format(epoch + 1, step + 1, end - start))
        model.save_pretrained(args.savepath + "/" + "model_e{}_s{}.pt".format(epoch + 1, step + 1))
