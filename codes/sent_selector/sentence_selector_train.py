import json
from pathlib import Path
from transformers import AutoTokenizer
import torch
from transformers import EncoderDecoderModel
from torch.utils.data import DataLoader
from transformers import AdamW
import numpy as np
from hanziconv import HanziConv
import argparse
import time
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments


## Retrieve load the dataset from the corresponding files
def get_dataset(sentlabelfile, contentfile):
    with open(sentlabelfile, "r", encoding="utf-8") as fd:
        sentlabels = [line.rstrip() for line in fd.readlines()]
        allsentbounds, allsentlabels = [], []
        for sentlabel in sentlabels:
            sentlabelsplitted = sentlabel.split(";")
            sentbounds = []
            sentlabels = []
            for info in sentlabelsplitted:
                bdl, bdh, label = info.split(",")
                sentbounds.append([int(bdl), int(bdh)])
                sentlabels.append(float(label))
            allsentbounds.append(sentbounds)
            allsentlabels.append(sentlabels)

    with open(contentfile, "r", encoding="utf-8") as fd:
        contents = [line.rstrip() for line in fd.readlines()]

    assert len(allsentlabels) == len(contents) and len(allsentlabels) == len(contents), \
        "Data Fields (Files) have different lengths"

    return allsentbounds, allsentlabels, contents


def show_answer(tokenizer_model, in_encodings, sentbound, sentlabel, idx):
    print("Index :: ", idx)
    print("Input (Content) :: ", tokenizer_model.decode(in_encodings['input_ids'][idx]))
    bod = in_encodings['input_ids'][idx].index(tokenizer.cls_token_id)
    eod = in_encodings['input_ids'][idx].index(tokenizer.sep_token_id)
    trimmed_text = in_encodings['input_ids'][idx][bod+1:eod]
    for bound, label in zip(sentbound[idx], sentlabel[idx]):
        print("Label (Do Select) :: ", label)
        print("Sentence :: ", tokenizer_model.decode(trimmed_text[bound[0]:bound[1]]))
    print("=======================================")


class GenerationDataset(torch.utils.data.Dataset):  # torch dataset object
    def __init__(self, input_encodings, sentbounds, sentlabels):  # takes in encoded datasets generated by huggingface tokenizers
        self.input_encodings = input_encodings

    def __getitem__(self, idx):  # for each idx, pass the input encodings and output encodings
        input_encodings_dict = {key: torch.tensor(val[idx]) for key, val in self.input_encodings.items()}
        input_encodings_dict["index"] = idx
        return input_encodings_dict

    def __len__(self):  # get the length (number of samples) in the dataset
        return len(self.input_encodings.input_ids)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Training Arguments")
    parser.add_argument("--contentfile", type=str, required=True,
                        help="Content (Article) File")
    parser.add_argument("--sentlabelfile", type=str, required=True,
                        help="Sentence Label File")
    parser.add_argument("--savepath", type=str, required=True, help="File")

    parser.add_argument("--batch_size", type=int, default=16, help="File")
    parser.add_argument("--epochs", type=int, default=25, help="File")
    parser.add_argument("--modelckpt", type=str, default="", help="File")
    parser.add_argument("--lr", type=float, default=1e-4, help="File")
    args = parser.parse_args()
    print("Get all Arguments:")
    for arg in vars(args):
        print("{}: {}".format(arg, getattr(args, arg)))

    modelbase = "bert-base-chinese"


    train_sentbounds, train_sentlabels, train_contents = get_dataset(args.sentlabelfile, args.contentfile)

    tokenizer = AutoTokenizer.from_pretrained(modelbase)
    tokenizer.add_special_tokens({"additional_special_tokens": ["[unused1]"]})
    train_input_encodings = tokenizer(train_contents, truncation=True, padding=True)

    print("Show some examples: ")
    show_answer(tokenizer, train_input_encodings, train_sentbounds, train_sentlabels, 0)
    show_answer(tokenizer, train_input_encodings, train_sentbounds, train_sentlabels, -1)
    print("Length of Train Set: {}".format(len(train_contents)))
    print("Done Dataset Processing")

    """The dataset is now ready for training"""
    train_dataset = GenerationDataset(train_input_encodings, train_sentbounds, train_sentlabels)
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)

    if args.modelckpt != "":
        print("Load from modelckpt: {}".format(args.modelckpt))
        model_encdec = EncoderDecoderModel.from_pretrained(args.modelckpt)
    else:
        print("Build from pretrained model: {}".format(modelbase))
        model_encdec = EncoderDecoderModel.from_encoder_decoder_pretrained(modelbase,
                                                                           modelbase)  # bert2bert, chinese variant
    # Create sentence scoring model
    for param in model_encdec.parameters(): # freeze everything
        param.requires_grad = False
    model_encoder = model_encdec.encoder
    model_sent_score = torch.nn.Sequential(torch.nn.Linear(768 * 2, 2),
                                           torch.nn.LogSoftmax(dim=1))# Pooler output + Sentence encoding via Mean-over-Position
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model_encoder.to(device)
    model_encoder.eval()
    model_sent_score.to(device)
    model_sent_score.train()
    print("Model Configuration")
    print(model_encoder.config)
    print("Model Architecture")
    print(model_encoder)

    loss_funct = torch.nn.NLLLoss()

    optim = AdamW(model_sent_score.parameters(), lr=args.lr)

    epoch, step = 0, 0
    for epoch in range(args.epochs):
        start = time.time()
        for step, batch_in in enumerate(train_loader):
            optim.zero_grad()
            input_ids = batch_in["input_ids"].to(device)
            attention_mask = batch_in["attention_mask"].to(device)
            outputs = model_encoder(input_ids=input_ids, attention_mask=attention_mask)
            assert len(outputs["last_hidden_state"]) == len(outputs["pooler_output"]) == len(batch_in["index"]), \
                "For the current batch, the information of concern are not of the same size." # chaining the length comparison
            in_batch_reconstitute = []
            out_batch_reconstitute = []
            # for _lhs, po, idx, intok in zip(outputs["last_hidden_state"], outputs["pooler_output"], batch_in["index"], batch_in["input_ids"]):
            #     intok = intok[1:]
            # for _lhs, po, idx in zip(outputs["last_hidden_state"], outputs["pooler_output"], batch_in["index"]):
            for _lhs, idx in zip(outputs["last_hidden_state"], batch_in["index"]):
                lhs = _lhs[1:] # remove CLS
                bounds = train_sentbounds[idx]
                labels = train_sentlabels[idx]
                content_emb = lhs[:bounds[-1][1]].mean(dim=0)
                for bound, label in zip(bounds, labels):
                    sent_embed = lhs[bound[0]:bound[1]].mean(axis=0)
                    in_batch_reconstitute.append(torch.cat([content_emb, sent_embed]).unsqueeze(0))
                    out_batch_reconstitute.append(label)
                    # print(tokenizer.decode(intok[bound[0]:bound[1]]))
            in_batch_reconstitute = torch.cat(in_batch_reconstitute, dim=0).to(device)
            out_batch_reconstitute = torch.LongTensor(out_batch_reconstitute).to(device)
            local_indices = torch.randperm(len(out_batch_reconstitute)) # idx shuffle
            local_indices = torch.split(local_indices, 32)
            for local_idx_block in local_indices:
                in_batch_block = in_batch_reconstitute[local_idx_block]
                out_batch_block = out_batch_reconstitute[local_idx_block]
                pred_scores = model_sent_score(in_batch_block)

                loss = loss_funct(pred_scores, out_batch_block)
                loss.backward()
                optim.step()
                print("Loss: {}, Acc {}:".format(loss, (pred_scores.argmax(dim=1)==out_batch_block).float().mean()))
                # print(pred_scores.argmax(dim=1))
                # print(out_batch_block)

            if ((step + 1) % 200) == 0:
                end = time.time()
                print(
                    "Epoch: {}, Step: {}, Duration (From Last Print-Out): {}".format(epoch + 1, step + 1, end - start))
                start = time.time()
        end = time.time()
        print("Epoch: {}, Step: {}, Duration (From Last Print-Out): {}".format(epoch + 1, step + 1, end - start))
        torch.save(model_sent_score.state_dict(), args.savepath + "/" + "model_sentsel_e{}_s{}.pt".format(epoch + 1, step + 1))
